{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAX7OtqLzseEf3/XjMhxO3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinay-852/Video_Synopsis/blob/main/Video_Synopsis(Clear).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xj2y8HxjyR5f"
      },
      "outputs": [],
      "source": [
        "#Copy data without downloading.......\n",
        "import os\n",
        "import requests\n",
        "import tqdm\n",
        "\n",
        "def text_retriever(url) -> list[str]:\n",
        "  \"\"\"\n",
        "  Retrieves URLs from a text file.\n",
        "\n",
        "  Args:\n",
        "      url (str): The URL of the text file containing video URLs.\n",
        "\n",
        "  Returns:\n",
        "      list[str]: A list of video URLs extracted from the text file.\n",
        "\n",
        "  Raises:\n",
        "      Exception: If there's an error retrieving or parsing the text file.\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an exception for non-2xx status codes\n",
        "\n",
        "    text = response.text\n",
        "    urls = text.splitlines()  # Split text into lines, removing unnecessary spaces\n",
        "\n",
        "    return urls\n",
        "  except Exception as e:\n",
        "    print(f\"Error retrieving URLs: {e}\")\n",
        "    return []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urls=text_retriever(\"https://raw.githubusercontent.com/Kitware/MEVID/refs/heads/main/mevid-v1-video-URLS.txt\")"
      ],
      "metadata": {
        "id": "7iEzg5ZHdcLr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download Dataset form internet.........\n",
        "import os\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "def download_file(url, save_dir):\n",
        "  \"\"\"\n",
        "  Download a file from a given URL and save it to a specified directory.\n",
        "\n",
        "  Args:\n",
        "    url: The URL of the file to download.\n",
        "    save_dir: The directory where the file should be saved.\n",
        "\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "  try:\n",
        "      if not os.path.exists(save_dir):\n",
        "          os.makedirs(save_dir)\n",
        "\n",
        "      filename = url.split('/')[-1]\n",
        "      file_path = os.path.join(save_dir, filename)\n",
        "\n",
        "      response = requests.get(url, stream=True)\n",
        "      total_size = int(response.headers.get('content-length', 0))\n",
        "\n",
        "      with open(file_path, 'wb') as f:\n",
        "          with tqdm(total=total_size, unit='B', unit_scale=True, desc='Downloading') as progress_bar:\n",
        "              for data in response.iter_content(chunk_size=1024):\n",
        "                  if data:\n",
        "                      progress_bar.update(len(data))\n",
        "                      f.write(data)\n",
        "\n",
        "      print(f\"Download complete! File saved to: {file_path}\")\n",
        "  except Exception as e:\n",
        "      print(f\"Error downloading file: {e}\")"
      ],
      "metadata": {
        "id": "uwd3EL1CydIL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_file(urls[0], \"/content/dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd7G10ARd1bN",
        "outputId": "d2fdc349-6790-41cf-e126-7f735d383548"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 162M/162M [00:18<00:00, 8.71MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete! File saved to: /content/dataset/2018-05-16.14-25-01.14-30-01.school.G639.r13.avi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q supervision\n",
        "!pip install -q git+https://github.com/THU-MIG/yolov10.git\n",
        "!pip install deep-sort-realtime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWcCqA-CytCd",
        "outputId": "5819185c-a4ed-4fc8-ff0c-79bf9c202c2c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/158.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m153.6/158.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ultralytics (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deep-sort-realtime\n",
            "  Downloading deep_sort_realtime-1.3.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deep-sort-realtime) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from deep-sort-realtime) (1.13.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from deep-sort-realtime) (4.10.0.84)\n",
            "Downloading deep_sort_realtime-1.3.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-sort-realtime\n",
            "Successfully installed deep-sort-realtime-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_file(\"https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10l.pt\",\"/content/weights\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IngPo5DnyvUP",
        "outputId": "929e38e9-61e5-454e-ec12-de7dcf6cd33f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 104M/104M [00:13<00:00, 7.99MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete! File saved to: /content/weights/yolov10l.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Video Format which can be played in Colab..............\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "def play_video(filename):\n",
        "  html = ''\n",
        "  video = open(filename,'rb').read()\n",
        "  src = 'data:video/mp4;base64,' + b64encode(video).decode()\n",
        "  html += fr'<video width=900 controls autoplay loop><source src=\"%s\" type=\"video/mp4\"></video>' % src\n",
        "  return HTML(html)"
      ],
      "metadata": {
        "id": "mTPcJaUFy_GW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display on Colab ........\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import subprocess\n",
        "\n",
        "def process_and_play_video(input_filename, output_filename):\n",
        "  \"\"\"\n",
        "  Uses the ffmpeg command to process a video file and play it in Colab.\n",
        "\n",
        "  Args:\n",
        "      input_filename (str): The path to the input video file.\n",
        "      output_filename (str): The path to save the processed video file.\n",
        "\n",
        "  Returns:\n",
        "      HTML: An HTML element to display the processed video in Colab.\n",
        "  \"\"\"\n",
        "  subprocess.run([\n",
        "        'ffmpeg',\n",
        "        '-hide_banner',\n",
        "        '-loglevel', 'error',\n",
        "        '-i', input_filename,\n",
        "        '-vcodec', 'libx264',\n",
        "        output_filename,\n",
        "        '-y'\n",
        "    ])\n",
        "\n",
        "  return play_video(output_filename)"
      ],
      "metadata": {
        "id": "wV2K9PAb-mq7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import datetime\n",
        "from absl import app, flags\n",
        "from absl.flags import FLAGS\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "from typing import Tuple, List\n",
        "import torch\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "y9p2Y6MGy4r9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#YOLOv10 model Operations..........\n",
        "from typing import Tuple, List\n",
        "import numpy as np\n",
        "from ultralytics import YOLOv10\n",
        "\n",
        "# Load YOLO model and COCO classes\n",
        "def load_yolo_model(model_file, class_file=\"coco.names\"):\n",
        "    \"\"\"\n",
        "    Loads the YOLO model for object detection and the class names from the COCO dataset.\n",
        "\n",
        "    Args:\n",
        "        model_file (str): Path to the YOLO model file.\n",
        "        class_file (str): Path to the file containing COCO class names.\n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "            - model (YOLOv10): The loaded YOLO model.\n",
        "            - class_names (list): List of class names corresponding to COCO dataset classes.\n",
        "    \"\"\"\n",
        "    # Check if a GPU is available; if not, use the CPU\n",
        "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    # Load the YOLO model\n",
        "    model = YOLOv10(model_file)\n",
        "\n",
        "    # Load COCO class names from the specified file\n",
        "    class_names=text_retriever(\"https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names\")\n",
        "\n",
        "    return model, class_names\n",
        "\n",
        "def detect_objects(model, frame, conf_threshold):\n",
        "    \"\"\"\n",
        "    Performs object detection on a given frame using the YOLO model.\n",
        "\n",
        "    Args:\n",
        "        model (YOLOv10): The loaded YOLO model used for detecting objects.\n",
        "        frame (numpy.ndarray): The input video frame to perform object detection on.\n",
        "        conf_threshold (float): Confidence threshold to filter weak detections.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of detections where each detection contains bounding box, confidence, and class ID.\n",
        "    \"\"\"\n",
        "    # Perform object detection on the frame\n",
        "    results = model(frame, verbose=False)[0]\n",
        "\n",
        "    detections = []\n",
        "    for det in results.boxes:\n",
        "        confidence = det.conf\n",
        "        label = det.cls\n",
        "        bbox = det.xyxy[0]  # Bounding box coordinates: [x1, y1, x2, y2]\n",
        "        x1, y1, x2, y2 = map(int, bbox)\n",
        "        class_id = int(label)\n",
        "\n",
        "        # Filter out weak detections based on confidence threshold\n",
        "        if confidence >= conf_threshold:\n",
        "            detections.append([[x1, y1, x2 - x1, y2 - y1], confidence, class_id])\n",
        "\n",
        "    return detections"
      ],
      "metadata": {
        "id": "H4goMRZxy9Wn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_file(\"https://i.cdn.newsbytesapp.com/images/l71620241008162609.jpeg\", \"/content/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8hYMHco2xyV",
        "outputId": "6769c470-9089-4b5c-9dc4-83cb905f2aa3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 132k/132k [00:00<00:00, 3.19MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete! File saved to: /content/l71620241008162609.jpeg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, class_names =load_yolo_model(\"/content/weights/yolov10l.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69UAX_Xe2fGA",
        "outputId": "798909c4-2283-4204-e9a7-87c690587e4f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py:733: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(file, map_location=\"cpu\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "detections = detect_objects(model, '/content/l71620241008162609.jpeg', conf_threshold=0.5)"
      ],
      "metadata": {
        "id": "dF8cq854maK5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(detections)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1ptEtA-214d",
        "outputId": "7d9af813-b766-49eb-bd95-2bb439973772"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[886, 78, 274, 968], tensor([0.9338]), 0], [[1477, 433, 136, 341], tensor([0.8929]), 0], [[941, 19, 306, 358], tensor([0.8528]), 34], [[1649, 543, 53, 138], tensor([0.8219]), 0], [[1745, 562, 47, 102], tensor([0.7588]), 0], [[1829, 554, 47, 122], tensor([0.7557]), 0], [[1874, 544, 45, 135], tensor([0.7498]), 0], [[1465, 526, 48, 53], tensor([0.7447]), 35], [[736, 583, 44, 73], tensor([0.5945]), 0], [[434, 573, 37, 82], tensor([0.5820]), 0], [[815, 592, 38, 64], tensor([0.5511]), 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def generate_background(video_file, output_file='background.jpg', method='median', num_frames=30):\n",
        "    \"\"\"\n",
        "    Generate a background image from the first 'num_frames' frames of a video by averaging or taking the median.\n",
        "\n",
        "    Args:\n",
        "        video_file (str): Path to the input video file.\n",
        "        output_file (str): Path where the background image will be saved.\n",
        "        method (str): Method to generate background ('mean' or 'median').\n",
        "        num_frames (int): Number of frames to use for background generation.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(video_file)\n",
        "\n",
        "    # Check if the video was opened successfully\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video file.\")\n",
        "        return\n",
        "\n",
        "    # Initialize a list to store frames\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "\n",
        "    # Read frames from the video until num_frames are collected\n",
        "    while frame_count < num_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            print(f\"End of video reached at frame {frame_count}.\")\n",
        "            break\n",
        "\n",
        "        # Append the frame to the list\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()\n",
        "\n",
        "    # Convert the list of frames to a numpy array\n",
        "    frames = np.array(frames)\n",
        "\n",
        "    # Generate the background by computing the median or mean of the frames\n",
        "    if method == 'median':\n",
        "        background = np.median(frames, axis=0).astype(dtype=np.uint8)  # Median\n",
        "    elif method == 'mean':\n",
        "        background = np.mean(frames, axis=0).astype(dtype=np.uint8)  # Mean\n",
        "    else:\n",
        "        print(\"Error: Method must be 'mean' or 'median'.\")\n",
        "        return\n",
        "\n",
        "    # Save the generated background as an image file\n",
        "    cv2.imwrite(output_file, background)\n",
        "    print(f\"Background image saved as {output_file}\")\n",
        "\n",
        "# Example usage:\n",
        "video_file = '/content/dataset/2018-05-16.14-25-01.14-30-01.school.G639.r13.avi'  # Replace with the path to your video file\n",
        "generate_background(video_file, output_file='background.jpg', method='median', num_frames=30)\n"
      ],
      "metadata": {
        "id": "4ZsveTMoAcic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "496c0d8e-fb9b-4a7d-cbf6-db47c2930bc3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Background image saved as background.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Deepsort Operations...........\n",
        "import numpy as np\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "\n",
        "def initialize_deepsort(max_age=20, n_init=3):\n",
        "    \"\"\"\n",
        "    Initializes the DeepSort tracker.\n",
        "\n",
        "    Args:\n",
        "        max_age (int): Maximum number of frames to keep alive a track.\n",
        "        n_init (int): Minimum number of detections before a track is confirmed.\n",
        "\n",
        "    Returns:\n",
        "        DeepSort: An instance of the DeepSort tracker.\n",
        "    \"\"\"\n",
        "    return DeepSort(max_age=max_age, n_init=n_init)\n",
        "\n",
        "def update_tracker(tracker, detections, frame):\n",
        "    \"\"\"\n",
        "    Updates the DeepSort tracker with new detections.\n",
        "\n",
        "    Args:\n",
        "        tracker (DeepSort): The DeepSort tracker instance.\n",
        "        detections (list): List of detections in the format [bounding_box, confidence, class_id].\n",
        "        frame (numpy.ndarray): The current video frame.\n",
        "\n",
        "    Returns:\n",
        "        List: Updated tracks from the DeepSort tracker.\n",
        "    \"\"\"\n",
        "    return tracker.update_tracks(detections, frame=frame)\n"
      ],
      "metadata": {
        "id": "6b09zbUQ3H2V"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Useful Utility function to complete tracking video.........\n",
        "import os\n",
        "import cv2\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize video capture and writer\n",
        "def initialize_video(video_file, output_file):\n",
        "    \"\"\"\n",
        "    Initializes video capture and writer objects for reading from the input video and writing to the output video.\n",
        "\n",
        "    Args:\n",
        "        video_file (str): Path to the input video file.\n",
        "        output_file (str): Path to the output video file where the processed video will be saved.\n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "            - video_cap (cv2.VideoCapture): The video capture object.\n",
        "            - writer (cv2.VideoWriter): The video writer object for saving processed frames.\n",
        "            - total_frames (int): Total number of frames in the input video.\n",
        "            - frame_width (int): Width of each frame in the video.\n",
        "            - frame_height (int): Height of each frame in the video.\n",
        "    \"\"\"\n",
        "    video_cap = cv2.VideoCapture(video_file)\n",
        "    frame_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(video_cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "    writer = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    total_frames = int(video_cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Total frames for tqdm\n",
        "    return video_cap, writer, total_frames, frame_width, frame_height\n",
        "\n",
        "\n",
        "def extract_hsv(frame, ltrb):\n",
        "    \"\"\"\n",
        "    Extracts the mean hue, saturation, and value (HSV) from the region of interest (ROI) defined by the bounding box.\n",
        "\n",
        "    Args:\n",
        "        frame (numpy.ndarray): The current video frame.\n",
        "        bbox (tuple): Bounding box coordinates (x1, y1, x2, y2) defining the ROI.\n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "            - mean_hue (float): Mean hue value in the ROI.\n",
        "            - mean_saturation (float): Mean saturation value in the ROI.\n",
        "            - mean_value (float): Mean value (brightness) in the ROI.\n",
        "    \"\"\"\n",
        "    x1, y1, x2, y2 = map(int, ltrb)\n",
        "    # Ensure the bounding box coordinates are within the frame boundaries\n",
        "    x1, y1, x2, y2 = max(0, x1), max(0, y1), min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
        "\n",
        "    if x2 > x1 and y2 > y1:\n",
        "        # Extract the region of interest (ROI) based on the bounding box\n",
        "        object_roi = frame[y1:y2, x1:x2]\n",
        "\n",
        "        # Convert the ROI to HSV color space\n",
        "        hsv_roi = cv2.cvtColor(object_roi, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "        # Compute the mean values for hue, saturation, and value in the HSV space\n",
        "        mean_hue = np.mean(hsv_roi[:, :, 0])\n",
        "        mean_saturation = np.mean(hsv_roi[:, :, 1])\n",
        "        mean_value = np.mean(hsv_roi[:, :, 2])\n",
        "\n",
        "        return mean_hue, mean_saturation, mean_value\n",
        "    return None, None, None\n",
        "\n",
        "# Write tracking data to CSV\n",
        "def write_csv(writer_csv, track_id, class_name, frame_count, bbox, hue, saturation, value):\n",
        "    \"\"\"\n",
        "    Writes the tracking data for an object to the CSV file, including track ID, class, frame number, bounding box, and HSV color information.\n",
        "\n",
        "    Args:\n",
        "        writer_csv (csv.writer): CSV writer object to save tracking data.\n",
        "        track_id (int): Unique ID for the tracked object.\n",
        "        class_name (str): Name of the detected object class.\n",
        "        frame_count (int): Current frame number.\n",
        "        bbox (tuple): Bounding box coordinates (x1, y1, x2, y2).\n",
        "        hue (float): Mean hue value of the object in the ROI.\n",
        "        saturation (float): Mean saturation value of the object in the ROI.\n",
        "        value (float): Mean value (brightness) of the object in the ROI.\n",
        "    \"\"\"\n",
        "    writer_csv.writerow([track_id, class_name, frame_count, bbox, hue, saturation, value])"
      ],
      "metadata": {
        "id": "u90rHqBH5Z9o"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def calculate_frame_difference(prev_frame, curr_frame):\n",
        "    \"\"\"\n",
        "    Calculate the difference between two frames using Mean Squared Error (MSE).\n",
        "\n",
        "    Args:\n",
        "        prev_frame (np.array): The previous frame.\n",
        "        curr_frame (np.array): The current frame.\n",
        "\n",
        "    Returns:\n",
        "        float: The difference between the two frames.\n",
        "    \"\"\"\n",
        "    # Convert frames to grayscale for simplicity\n",
        "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "    curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Compute Mean Squared Error (MSE) between the two frames\n",
        "    diff = np.sum((prev_gray.astype(\"float\") - curr_gray.astype(\"float\")) ** 2)\n",
        "    diff /= float(prev_gray.shape[0] * prev_gray.shape[1])\n",
        "\n",
        "    return diff\n",
        "\n"
      ],
      "metadata": {
        "id": "aSRDlDIilTsK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import csv\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "\n",
        "def extract_object_tracks(model_file, video_file, output_file, conf_threshold, threshold, crops_dir=\"object_crops\"):\n",
        "    \"\"\"\n",
        "    Main function to extract object tracks from key frames of a video using YOLO for object detection and\n",
        "    DeepSort for tracking. The function saves the tracking data, including bounding boxes, HSV values, and\n",
        "    cropped objects to a CSV file, and generates an output video with annotated tracks.\n",
        "\n",
        "    Args:\n",
        "        model_file (str): Path to the YOLO model file.\n",
        "        video_file (str): Path to the input video file.\n",
        "        output_file (str): Path to the output video file to save the annotated video.\n",
        "        conf_threshold (float): Confidence threshold to filter weak detections.\n",
        "        threshold (float): Threshold for selecting key frames based on frame difference.\n",
        "        crops_dir (str): Directory where cropped object images will be saved.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize video capture, writer, and other video properties\n",
        "        video_cap, writer, total_frames, frame_width, frame_height = initialize_video(video_file, output_file)\n",
        "\n",
        "        # Load YOLO model and COCO class names\n",
        "        model, class_names = load_yolo_model(model_file)\n",
        "\n",
        "        # Initialize DeepSort tracker\n",
        "        tracker = DeepSort(max_age=20, n_init=3)\n",
        "\n",
        "        # Create directory for saving crops if it doesn't exist\n",
        "        if not os.path.exists(crops_dir):\n",
        "            os.makedirs(crops_dir)\n",
        "\n",
        "        # Open CSV file for saving object tracks\n",
        "        with open('object_tracks.csv', mode='w', newline='') as file:\n",
        "            writer_csv = csv.writer(file)\n",
        "            writer_csv.writerow(['Track ID', 'Class Name', 'Frame', 'Bounding Box', 'Hue', 'Saturation', 'Value', 'Crop Path'])\n",
        "\n",
        "            frame_count = 0\n",
        "            prev_frame = None\n",
        "\n",
        "            # Progress bar for processing video frames\n",
        "            with tqdm(total=total_frames, desc=\"Processing Frames\") as pbar:\n",
        "                while True:\n",
        "                    ret, frame = video_cap.read()\n",
        "                    frame_count += 1\n",
        "                    if not ret:\n",
        "                        print(\"End of video.\")\n",
        "                        break\n",
        "\n",
        "                    # If this is the first frame, store it and continue to the next\n",
        "                    if prev_frame is None:\n",
        "                        prev_frame = frame\n",
        "                        continue\n",
        "\n",
        "                    # Calculate the difference between the current frame and the previous frame\n",
        "                    frame_diff = calculate_frame_difference(prev_frame, frame)\n",
        "\n",
        "                    # Check if the difference exceeds the threshold (key frame selection)\n",
        "                    if frame_diff > threshold:\n",
        "                        print(f\"Processing key frame {frame_count} (difference: {frame_diff})\")\n",
        "\n",
        "                        # Object detection\n",
        "                        detections = detect_objects(model, frame, conf_threshold)\n",
        "                        tracks = update_tracker(tracker, detections, frame)\n",
        "\n",
        "                        for track in tracks:\n",
        "                            if track.is_confirmed():\n",
        "                                track_id = track.track_id\n",
        "                                ltrb = track.to_ltrb()  # Bounding box format: (left, top, right, bottom)\n",
        "                                class_id = track.get_det_class()\n",
        "                                class_name = class_names[class_id]\n",
        "\n",
        "                                # Extract HSV color features from the bounding box region\n",
        "                                hue, saturation, value = extract_hsv(frame, ltrb)\n",
        "\n",
        "                                # Crop the object and save the cropped image\n",
        "                                x1, y1, x2, y2 = map(int, ltrb)\n",
        "                                crop_img = frame[y1:y2, x1:x2]\n",
        "                                crop_path = f\"{crops_dir}/track_{track_id}_frame_{frame_count}.png\"\n",
        "                                cv2.imwrite(crop_path, crop_img)\n",
        "\n",
        "                                if hue is not None:\n",
        "                                    # Write the tracking data (including HSV and crop path) to the CSV file\n",
        "                                    writer_csv.writerow([track_id, class_name, frame_count, ltrb, hue, saturation, value, crop_path])\n",
        "\n",
        "                                    # Draw bounding box and label on the frame\n",
        "                                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                                    cv2.putText(frame, f'{class_name} {track_id}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
        "\n",
        "                        # Write the frame with annotations to the output video\n",
        "                        writer.write(frame)\n",
        "\n",
        "                    # Update the previous frame\n",
        "                    prev_frame = frame\n",
        "                    pbar.update(1)\n",
        "\n",
        "        # Release video capture and writer resources\n",
        "        video_cap.release()\n",
        "        writer.release()\n",
        "        print(f\"Tracking completed and saved to 'object_tracks.csv'. Cropped images saved to '{crops_dir}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Example usage of the function with keyframe selection and cropping\n",
        "model_file = \"weights/yolov10l.pt\"\n",
        "video_file = \"/content/dataset/2018-05-16.14-25-01.14-30-01.school.G639.r13.avi\"\n",
        "output_file = \"output_tracked_keyframes.mp4\"\n",
        "conf_threshold = 0.5\n",
        "frame_diff_threshold = 6  # Set a threshold for keyframe selection\n",
        "\n",
        "# Call the main function to extract object tracks based on key frames and crop objects\n",
        "extract_object_tracks(model_file, video_file, output_file, conf_threshold, frame_diff_threshold)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIkjKMA5lQvj",
        "outputId": "dff72356-9b34-465f-9bf5-0c343d41d846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ultralytics/nn/tasks.py:733: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(file, map_location=\"cpu\")\n",
            "/usr/local/lib/python3.10/dist-packages/deep_sort_realtime/embedder/embedder_pytorch.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.model.load_state_dict(torch.load(model_wts_path))\n",
            "Processing Frames:   1%|          | 59/9001 [00:04<11:26, 13.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 61 (difference: 12.92500048585199)\n",
            "Processing key frame 62 (difference: 16.59521047108209)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Frames:   1%|▏         | 118/9001 [00:10<05:14, 28.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 121 (difference: 15.31318116449005)\n",
            "Processing key frame 122 (difference: 16.554047632929105)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Frames:   2%|▏         | 177/9001 [00:16<05:51, 25.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 181 (difference: 14.726340465640547)\n",
            "Processing key frame 182 (difference: 15.990214940920398)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Frames:   2%|▏         | 210/9001 [00:19<07:39, 19.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 214 (difference: 7.127744577891791)\n",
            "Processing key frame 215 (difference: 6.974969391324627)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Frames:   3%|▎         | 236/9001 [00:23<10:39, 13.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 241 (difference: 12.969013817630596)\n",
            "Processing key frame 242 (difference: 14.397127157182835)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Frames:   3%|▎         | 246/9001 [00:27<30:47,  4.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 252 (difference: 7.478159981343284)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Frames:   3%|▎         | 294/9001 [00:31<10:36, 13.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 301 (difference: 14.41054298818408)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   3%|▎         | 300/9001 [00:32<19:04,  7.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 302 (difference: 16.064321944962686)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Frames:   3%|▎         | 305/9001 [00:34<30:17,  4.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 312 (difference: 6.064753381529851)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Frames:   4%|▍         | 357/9001 [00:37<04:04, 35.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 361 (difference: 13.361179940143035)\n",
            "Processing key frame 362 (difference: 15.253846004353234)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Frames:   5%|▍         | 416/9001 [00:43<04:02, 35.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 421 (difference: 13.69697314210199)\n",
            "Processing key frame 422 (difference: 15.158663226834577)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Frames:   5%|▌         | 475/9001 [00:49<09:02, 15.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 481 (difference: 13.845815842661692)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   5%|▌         | 480/9001 [00:51<23:58,  5.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 482 (difference: 15.024432039023631)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Frames:   6%|▌         | 531/9001 [00:55<04:34, 30.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 538 (difference: 6.084906036225124)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▌         | 537/9001 [00:57<14:09,  9.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 539 (difference: 6.87743946284204)\n",
            "Processing key frame 541 (difference: 20.28954349347015)\n",
            "Processing key frame 542 (difference: 31.352378245491295)\n",
            "Processing key frame 543 (difference: 17.781996754508707)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▌         | 542/9001 [01:06<1:14:07,  1.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 544 (difference: 16.587871676772387)\n",
            "Processing key frame 545 (difference: 15.208632618159204)\n",
            "Processing key frame 546 (difference: 17.0714012943097)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▌         | 545/9001 [01:12<1:48:16,  1.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 547 (difference: 16.771897835043532)\n",
            "Processing key frame 548 (difference: 16.60292920164801)\n",
            "Processing key frame 549 (difference: 16.888838036380598)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▌         | 548/9001 [01:16<2:10:50,  1.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 550 (difference: 17.49278461209577)\n",
            "Processing key frame 551 (difference: 16.850824490827115)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▌         | 550/9001 [01:20<2:31:47,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 552 (difference: 15.904508706467661)\n",
            "Processing key frame 553 (difference: 15.965892704446517)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▌         | 552/9001 [01:24<2:53:05,  1.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 554 (difference: 15.663051442008706)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▌         | 553/9001 [01:25<2:57:39,  1.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 555 (difference: 15.164989019745025)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▌         | 554/9001 [01:27<3:04:06,  1.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 556 (difference: 14.562423235385571)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▌         | 555/9001 [01:28<3:08:20,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 557 (difference: 15.080016907649254)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▌         | 556/9001 [01:30<3:14:00,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 558 (difference: 15.55097899175995)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▌         | 557/9001 [01:31<3:16:28,  1.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 559 (difference: 15.760095032649254)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▌         | 558/9001 [01:34<3:53:33,  1.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 560 (difference: 16.39590183846393)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▌         | 559/9001 [01:37<4:36:08,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 561 (difference: 16.032865943718907)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▌         | 560/9001 [01:39<5:01:56,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 562 (difference: 16.055697100435324)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▌         | 561/9001 [01:43<5:58:35,  2.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 563 (difference: 16.540159553793533)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▌         | 562/9001 [01:47<7:15:06,  3.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 564 (difference: 16.976396824471394)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 563/9001 [01:50<6:44:14,  2.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 565 (difference: 17.565672613495025)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 564/9001 [01:52<6:08:40,  2.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 566 (difference: 17.808529131685322)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 565/9001 [01:54<5:41:58,  2.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 567 (difference: 17.141943602300994)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 566/9001 [01:55<5:01:47,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 568 (difference: 17.16374621035448)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 567/9001 [01:57<4:35:44,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 569 (difference: 17.283171544620647)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 568/9001 [01:59<5:00:19,  2.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 570 (difference: 19.401717486784825)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 569/9001 [02:02<5:09:27,  2.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 571 (difference: 20.189289878731344)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 570/9001 [02:03<4:41:45,  2.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 572 (difference: 20.960140702736318)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 571/9001 [02:05<4:25:29,  1.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 573 (difference: 22.96270162857587)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 572/9001 [02:07<4:32:22,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 574 (difference: 33.629368295242536)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 573/9001 [02:09<4:19:23,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 575 (difference: 34.475360988028605)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 574/9001 [02:11<4:43:58,  2.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 576 (difference: 34.02633755052861)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 575/9001 [02:13<4:54:49,  2.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 577 (difference: 33.588044640080845)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 576/9001 [02:15<5:00:06,  2.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 578 (difference: 33.72925703513682)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 577/9001 [02:17<4:42:15,  2.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 579 (difference: 32.92950530550373)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 578/9001 [02:19<4:26:37,  1.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 580 (difference: 33.05694622590174)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 579/9001 [02:21<4:30:40,  1.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 581 (difference: 34.75685731498756)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 580/9001 [02:23<4:35:58,  1.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 582 (difference: 36.11538304570895)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 581/9001 [02:25<4:45:35,  2.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 583 (difference: 35.969047827269904)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 582/9001 [02:28<5:04:25,  2.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 584 (difference: 35.74189307369403)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 583/9001 [02:30<5:10:15,  2.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 585 (difference: 35.71590922341418)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 584/9001 [02:32<5:01:26,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 586 (difference: 34.47232295553483)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   6%|▋         | 585/9001 [02:33<4:35:48,  1.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 587 (difference: 31.836162935323383)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   7%|▋         | 586/9001 [02:35<4:18:38,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 588 (difference: 32.170146824471395)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   7%|▋         | 587/9001 [02:36<4:06:01,  1.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 589 (difference: 33.5536720693408)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   7%|▋         | 588/9001 [02:39<4:48:53,  2.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 590 (difference: 32.484259367226365)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   7%|▋         | 589/9001 [02:41<4:54:53,  2.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 591 (difference: 31.365024001088308)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   7%|▋         | 590/9001 [02:44<5:04:11,  2.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 592 (difference: 30.104435342817165)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   7%|▋         | 591/9001 [02:45<4:38:31,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 593 (difference: 30.62266159437189)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   7%|▋         | 592/9001 [02:47<4:20:42,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 594 (difference: 31.673902460354476)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   7%|▋         | 593/9001 [02:48<4:08:27,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 595 (difference: 28.9401270017102)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   7%|▋         | 594/9001 [02:51<4:39:58,  2.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 596 (difference: 27.858810925839553)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   7%|▋         | 595/9001 [02:54<5:04:20,  2.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 597 (difference: 27.825897854477613)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   7%|▋         | 596/9001 [02:56<5:05:01,  2.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 598 (difference: 27.11982810556592)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   7%|▋         | 597/9001 [02:57<4:38:52,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 599 (difference: 27.734794290267413)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   7%|▋         | 598/9001 [02:59<4:21:52,  1.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing key frame 600 (difference: 26.48828076414801)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Frames:   7%|▋         | 599/9001 [03:00<4:05:38,  1.75s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import math\n",
        "\n",
        "def calculate_area_velocity_tubes(input_csv, output_csv):\n",
        "    \"\"\"\n",
        "    Calculate area and velocity for each object in the object_tracks.csv file by considering objects in the same tube\n",
        "    (i.e., same Track ID) and computing the velocity based on the movement between consecutive frames.\n",
        "\n",
        "    Args:\n",
        "        input_csv (str): Path to the input CSV file (object_tracks.csv).\n",
        "        output_csv (str): Path to the output CSV file to save the results with area and velocity.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        rows = []\n",
        "\n",
        "        # Read the input CSV file\n",
        "        with open(input_csv, 'r') as file:\n",
        "            reader = csv.DictReader(file)\n",
        "            for row in reader:\n",
        "                # Convert frame to int and bounding box values to float for calculations\n",
        "                row['Frame'] = int(row['Frame'])\n",
        "\n",
        "                # Parse the bounding box values by removing brackets and splitting by commas or spaces\n",
        "                bbox_str = row['Bounding Box'].strip('[]')\n",
        "                row['Bounding Box'] = tuple(map(float, bbox_str.replace(\",\", \" \").split()))\n",
        "\n",
        "                rows.append(row)\n",
        "\n",
        "        # Sort rows by Track ID and Frame to ensure sequential processing\n",
        "        rows.sort(key=lambda x: (int(x['Track ID']), x['Frame']))\n",
        "\n",
        "        # Open the output CSV file for writing\n",
        "        with open(output_csv, 'w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            # Write the new header with Area and Velocity columns\n",
        "            writer.writerow(['Track ID', 'Class Name', 'Frame', 'Bounding Box', 'Hue', 'Saturation', 'Value', 'Area', 'Velocity'])\n",
        "\n",
        "            prev_objects = {}  # To store the last seen object positions per Track ID\n",
        "\n",
        "            # Loop through the rows and calculate area and velocity\n",
        "            for row in rows:\n",
        "                try:\n",
        "                    # Extract bounding box values\n",
        "                    left, top, right, bottom = row['Bounding Box']\n",
        "\n",
        "                    # Calculate area of the bounding box\n",
        "                    area = (right - left) * (bottom - top)\n",
        "\n",
        "                    # Calculate velocity for the same Track ID between consecutive frames\n",
        "                    track_id = row['Track ID']\n",
        "                    current_frame = row['Frame']\n",
        "\n",
        "                    # Calculate center of the current bounding box\n",
        "                    center_x = (left + right) / 2\n",
        "                    center_y = (top + bottom) / 2\n",
        "\n",
        "                    # Check if we have a previous record for the same track_id (same tube)\n",
        "                    if track_id in prev_objects:\n",
        "                        prev_frame, (prev_center_x, prev_center_y) = prev_objects[track_id]\n",
        "\n",
        "                        # Calculate velocity if the object exists in the previous frame (adjacent frames only)\n",
        "                        if current_frame == prev_frame + 1:\n",
        "                            velocity = math.sqrt((center_x - prev_center_x) ** 2 + (center_y - prev_center_y) ** 2)\n",
        "                        else:\n",
        "                            velocity = 0  # If not consecutive frames, velocity is 0\n",
        "                    else:\n",
        "                        velocity = 0  # First appearance of the object, velocity is 0\n",
        "\n",
        "                    # Update the previous object record with the current frame and center\n",
        "                    prev_objects[track_id] = (current_frame, (center_x, center_y))\n",
        "\n",
        "                    # Write the row to the new CSV file with calculated area and velocity\n",
        "                    writer.writerow([\n",
        "                        row['Track ID'], row['Class Name'], row['Frame'], row['Bounding Box'],\n",
        "                        row['Hue'], row['Saturation'], row['Value'], area, velocity\n",
        "                    ])\n",
        "\n",
        "                except Exception as row_error:\n",
        "                    print(f\"Error processing row: {row}, error: {row_error}\")\n",
        "                    continue  # Skip any problematic rows\n",
        "\n",
        "        print(f\"Area and velocity calculations completed and saved to {output_csv}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Call the function with the input and output CSV file paths\n",
        "input_csv = 'object_tracks.csv'\n",
        "output_csv = 'object_tracks_with_area_velocity_tubes.csv'\n",
        "\n",
        "calculate_area_velocity_tubes(input_csv, output_csv)\n"
      ],
      "metadata": {
        "id": "WVc6Xitrkd7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('object_tracks_with_area_velocity_tubes.csv')\n",
        "df.head(50)"
      ],
      "metadata": {
        "id": "b5s_ZTJNnFnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "from copy import deepcopy\n",
        "\n",
        "# Load database from CSV file with additional object information\n",
        "def load_tracking_database(csv_file):\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Clean up the 'Bounding Box' column and convert to list of coordinates\n",
        "    df['Bounding Box'] = df['Bounding Box'].apply(lambda x: [int(coord) for coord in x.strip('()').split(',')])\n",
        "\n",
        "    # Filter to only include persons\n",
        "    df = df[df['Class Name'] == 'person']\n",
        "\n",
        "    # Convert dataframe to list of dictionaries\n",
        "    database = df.to_dict(orient='records')\n",
        "    return database\n",
        "\n",
        "# Group the tracking data by Track ID\n",
        "def group_tracks_by_id(database):\n",
        "    grouped_tracks = {}\n",
        "    for record in database:\n",
        "        track_id = record['Track ID']\n",
        "        if track_id not in grouped_tracks:\n",
        "            grouped_tracks[track_id] = []\n",
        "        grouped_tracks[track_id].append(record)\n",
        "    return list(grouped_tracks.values())\n",
        "\n",
        "# Utility to compute Intersection over Union (IoU)\n",
        "def iou(box1, box2):\n",
        "    x1, y1, x2, y2 = box1\n",
        "    x1_b, y1_b, x2_b, y2_b = box2\n",
        "\n",
        "    # Calculate intersection\n",
        "    inter_x1 = max(x1, x1_b)\n",
        "    inter_y1 = max(y1, y1_b)\n",
        "    inter_x2 = min(x2, x2_b)\n",
        "    inter_y2 = min(y2, y2_b)\n",
        "\n",
        "    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
        "    box1_area = (x2 - x1) * (y2 - y1)\n",
        "    box2_area = (x2_b - x1_b) * (y2_b - y1_b)\n",
        "    union_area = box1_area + box2_area - inter_area\n",
        "\n",
        "    return inter_area / union_area if union_area != 0 else 0\n",
        "\n",
        "# Temporal coherence: smoothness of bounding box motion over frames\n",
        "def temporal_coherence(tracks):\n",
        "    coherence_score = 0\n",
        "    for track in tracks:\n",
        "        for i in range(1, len(track)):\n",
        "            box_prev = track[i-1]['Bounding Box']\n",
        "            box_curr = track[i]['Bounding Box']\n",
        "            displacement = np.linalg.norm(np.array(box_prev[:2]) - np.array(box_curr[:2]))\n",
        "            coherence_score += np.exp(-displacement)\n",
        "    return coherence_score\n",
        "\n",
        "# Spatial coherence: minimize overlap between bounding boxes in the same frame\n",
        "def spatial_coherence(tracks):\n",
        "    overlap_penalty = 0\n",
        "    frames = {}\n",
        "    for track in tracks:\n",
        "        for frame_info in track:\n",
        "            frame = frame_info['Frame']\n",
        "            box = frame_info['Bounding Box']\n",
        "            if frame not in frames:\n",
        "                frames[frame] = []\n",
        "            frames[frame].append(box)\n",
        "\n",
        "    for boxes in frames.values():\n",
        "        for i in range(len(boxes)):\n",
        "            for j in range(i + 1, len(boxes)):\n",
        "                overlap_penalty += max(0, iou(boxes[i], boxes[j]) - 0.1)  # Penalize for high overlap\n",
        "\n",
        "    return overlap_penalty\n",
        "\n",
        "# Fitness function with temporal and spatial coherence\n",
        "def fitness(tracks, w1=1.5, w2=2):\n",
        "    return w1 * temporal_coherence(tracks) - w2 * spatial_coherence(tracks)\n",
        "\n",
        "# Scale down the bounding box to reduce chances of overlap\n",
        "def scale_bbox(bbox, scale_factor=0.5):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    width = x2 - x1\n",
        "    height = y2 - y1\n",
        "    center_x = (x1 + x2) / 2\n",
        "    center_y = (y1 + y2) / 2\n",
        "\n",
        "    new_width = width * scale_factor\n",
        "    new_height = height * scale_factor\n",
        "\n",
        "    new_x1 = center_x - new_width / 2\n",
        "    new_y1 = center_y - new_height / 2\n",
        "    new_x2 = center_x + new_width / 2\n",
        "    new_y2 = center_y + new_height / 2\n",
        "\n",
        "    return [new_x1, new_y1, new_x2, new_y2]\n",
        "\n",
        "# Shift bounding boxes slightly to reduce overlap\n",
        "def shift_bbox(bbox, shift_value=2):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    return [x1 + shift_value, y1 + shift_value, x2 + shift_value, y2 + shift_value]\n",
        "\n",
        "# Adjust BBoxes in a frame to minimize overlap\n",
        "def adjust_frame_bboxes(frame_boxes):\n",
        "    adjusted_boxes = []\n",
        "    for i in range(len(frame_boxes)):\n",
        "        for j in range(i + 1, len(frame_boxes)):\n",
        "            if iou(frame_boxes[i], frame_boxes[j]) > 0:\n",
        "                # If overlapping, apply a shift\n",
        "                frame_boxes[j] = shift_bbox(frame_boxes[j])\n",
        "        adjusted_boxes.append(scale_bbox(frame_boxes[i]))\n",
        "    return adjusted_boxes\n",
        "\n",
        "# Mutate by scaling and shifting bounding boxes to avoid overlaps\n",
        "def mutate(individual, mutation_rate=0.05, max_shift=3, max_bbox_shift=3):\n",
        "    mutated = deepcopy(individual)\n",
        "    frames = {}\n",
        "\n",
        "    for track in mutated:\n",
        "        for obj in track:\n",
        "            frame = obj['Frame']\n",
        "            if frame not in frames:\n",
        "                frames[frame] = []\n",
        "            frames[frame].append(obj['Bounding Box'])\n",
        "\n",
        "    # Adjust BBoxes in each frame to avoid overlap\n",
        "    for frame, boxes in frames.items():\n",
        "        frames[frame] = adjust_frame_bboxes(boxes)\n",
        "\n",
        "    # Apply adjusted BBoxes back to tracks\n",
        "    for track in mutated:\n",
        "        for obj in track:\n",
        "            frame = obj['Frame']\n",
        "            obj['Bounding Box'] = frames[frame].pop(0)\n",
        "\n",
        "    return mutated\n",
        "\n",
        "# Simulated Annealing with track shifting and enhanced mutation\n",
        "def simulated_annealing(database, initial_temperature=1000, cooling_rate=0.995, mutation_rate=0.1, max_generations=100):\n",
        "    # Initialize population with a single solution\n",
        "    current_solution = deepcopy(database)\n",
        "\n",
        "    current_fitness = fitness(current_solution)\n",
        "    temperature = initial_temperature\n",
        "\n",
        "    for generation in range(max_generations):\n",
        "        # Mutate the current solution\n",
        "        new_solution = mutate(current_solution, mutation_rate)\n",
        "        new_fitness = fitness(new_solution)\n",
        "\n",
        "        # Calculate the acceptance probability\n",
        "        if new_fitness > current_fitness:\n",
        "            current_solution = new_solution\n",
        "            current_fitness = new_fitness\n",
        "        else:\n",
        "            acceptance_probability = math.exp((new_fitness - current_fitness) / temperature)\n",
        "            if random.random() < acceptance_probability:\n",
        "                current_solution = new_solution\n",
        "                current_fitness = new_fitness\n",
        "\n",
        "        # Cool down the temperature\n",
        "        temperature *= cooling_rate\n",
        "        if generation % 50 == 0 or generation == max_generations - 1:\n",
        "            print(f\"Generation {generation}, Fitness: {current_fitness:.4f}, Temperature: {temperature:.2f}\")\n",
        "\n",
        "    return current_solution\n",
        "\n",
        "# Main script to load the CSV and run simulated annealing\n",
        "if __name__ == \"__main__\":\n",
        "    csv_file = '/content/object_tracks.csv'  # Replace with the path to your CSV file\n",
        "    tracking_database = load_tracking_database(csv_file)\n",
        "\n",
        "    # Group the tracks by Track ID\n",
        "    grouped_tracks = group_tracks_by_id(tracking_database)\n",
        "\n",
        "    # Run simulated annealing on the grouped tracks\n",
        "    best_tracks = simulated_annealing(grouped_tracks)\n",
        "\n",
        "    # Flatten the list of tracks for output\n",
        "    optimized_data = []\n",
        "    for track in best_tracks:\n",
        "        optimized_data.extend(track)\n",
        "\n",
        "    # Convert back to DataFrame for saving or further processing\n",
        "    optimized_df = pd.DataFrame(optimized_data)\n",
        "    optimized_df.to_csv('/content/optimized_person_tracks.csv', index=False)\n",
        "\n",
        "    print(\"Optimized Person Tracking Data Saved to 'optimized_person_tracks.csv'\")\n",
        "    print(optimized_df.head())\n"
      ],
      "metadata": {
        "id": "cIwYX2dOOsX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "from copy import deepcopy\n",
        "\n",
        "# Load database from CSV file with additional object information\n",
        "def load_tracking_database(csv_file):\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Clean up the 'Bounding Box' column and convert to list of coordinates\n",
        "    df['Bounding Box'] = df['Bounding Box'].apply(lambda x: [int(coord) for coord in x.strip('[]').split(',')])\n",
        "\n",
        "    # Filter to only include persons\n",
        "    df = df[df['Class Name'] == 'person']\n",
        "\n",
        "    # Convert dataframe to list of dictionaries\n",
        "    database = df.to_dict(orient='records')\n",
        "    return database\n",
        "\n",
        "# Group the tracking data by Track ID\n",
        "def group_tracks_by_id(database):\n",
        "    grouped_tracks = {}\n",
        "    for record in database:\n",
        "        track_id = record['Track ID']\n",
        "        if track_id not in grouped_tracks:\n",
        "            grouped_tracks[track_id] = []\n",
        "        grouped_tracks[track_id].append(record)\n",
        "    return list(grouped_tracks.values())\n",
        "\n",
        "# Utility to compute Intersection over Union (IoU)\n",
        "def iou(box1, box2):\n",
        "    x1, y1, x2, y2 = box1\n",
        "    x1_b, y1_b, x2_b, y2_b = box2\n",
        "\n",
        "    # Calculate intersection\n",
        "    inter_x1 = max(x1, x1_b)\n",
        "    inter_y1 = max(y1, y1_b)\n",
        "    inter_x2 = min(x2, x2_b)\n",
        "    inter_y2 = min(y2, y2_b)\n",
        "\n",
        "    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
        "    box1_area = (x2 - x1) * (y2 - y1)\n",
        "    box2_area = (x2_b - x1_b) * (y2_b - y1_b)\n",
        "    union_area = box1_area + box2_area - inter_area\n",
        "\n",
        "    return inter_area / union_area if union_area != 0 else 0\n",
        "\n",
        "# Temporal coherence: smoothness of bounding box motion over frames\n",
        "def temporal_coherence(tracks):\n",
        "    coherence_score = 0\n",
        "    for track in tracks:\n",
        "        for i in range(1, len(track)):\n",
        "            box_prev = track[i-1]['Bounding Box']\n",
        "            box_curr = track[i]['Bounding Box']\n",
        "            displacement = np.linalg.norm(np.array(box_prev[:2]) - np.array(box_curr[:2]))\n",
        "            coherence_score += np.exp(-displacement)\n",
        "    return coherence_score\n",
        "\n",
        "# Spatial coherence: minimize overlap between bounding boxes in the same frame\n",
        "def spatial_coherence(tracks):\n",
        "    overlap_penalty = 0\n",
        "    frames = {}\n",
        "    for track in tracks:\n",
        "        for frame_info in track:\n",
        "            frame = frame_info['Frame']\n",
        "            box = frame_info['Bounding Box']\n",
        "            if frame not in frames:\n",
        "                frames[frame] = []\n",
        "            frames[frame].append(box)\n",
        "\n",
        "    for boxes in frames.values():\n",
        "        for i in range(len(boxes)):\n",
        "            for j in range(i + 1, len(boxes)):\n",
        "                overlap_penalty += max(0, iou(boxes[i], boxes[j]) - 0.1)  # Penalize for high overlap\n",
        "\n",
        "    return overlap_penalty\n",
        "\n",
        "# Fitness function with temporal and spatial coherence\n",
        "def fitness(tracks, w1=1.5, w2=2):\n",
        "    return w1 * temporal_coherence(tracks) - w2 * spatial_coherence(tracks)\n",
        "\n",
        "# Calculate area of a bounding box\n",
        "def calculate_area(bbox):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    return (x2 - x1) * (y2 - y1)\n",
        "\n",
        "# Calculate velocity based on consecutive frames for each track\n",
        "def calculate_velocity(track):\n",
        "    prev_frame, prev_center = None, None\n",
        "    for obj in track:\n",
        "        current_bbox = obj['Bounding Box']\n",
        "        current_frame = obj['Frame']\n",
        "\n",
        "        # Calculate center of the current bounding box\n",
        "        center_x = (current_bbox[0] + current_bbox[2]) / 2\n",
        "        center_y = (current_bbox[1] + current_bbox[3]) / 2\n",
        "\n",
        "        if prev_frame is not None and current_frame == prev_frame + 1:\n",
        "            # Calculate velocity if it's the next frame in the sequence\n",
        "            velocity = math.sqrt((center_x - prev_center[0]) ** 2 + (center_y - prev_center[1]) ** 2)\n",
        "        else:\n",
        "            # First frame or non-consecutive frames\n",
        "            velocity = 0\n",
        "\n",
        "        # Store velocity and area in the object\n",
        "        obj['Area'] = calculate_area(current_bbox)\n",
        "        obj['Velocity'] = velocity\n",
        "\n",
        "        # Update previous frame and center\n",
        "        prev_frame = current_frame\n",
        "        prev_center = (center_x, center_y)\n",
        "\n",
        "    return track\n",
        "\n",
        "# Scale down the bounding box to reduce chances of overlap\n",
        "def scale_bbox(bbox, scale_factor=0.5):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    width = x2 - x1\n",
        "    height = y2 - y1\n",
        "    center_x = (x1 + x2) / 2\n",
        "    center_y = (y1 + y2) / 2\n",
        "\n",
        "    new_width = width * scale_factor\n",
        "    new_height = height * scale_factor\n",
        "\n",
        "    new_x1 = center_x - new_width / 2\n",
        "    new_y1 = center_y - new_height / 2\n",
        "    new_x2 = center_x + new_width / 2\n",
        "    new_y2 = center_y + new_height / 2\n",
        "\n",
        "    return [new_x1, new_y1, new_x2, new_y2]\n",
        "\n",
        "# Shift bounding boxes slightly to reduce overlap\n",
        "def shift_bbox(bbox, shift_value=2):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    return [x1 + shift_value, y1 + shift_value, x2 + shift_value, y2 + shift_value]\n",
        "\n",
        "# Adjust BBoxes in a frame to minimize overlap\n",
        "def adjust_frame_bboxes(frame_boxes):\n",
        "    adjusted_boxes = []\n",
        "    for i in range(len(frame_boxes)):\n",
        "        for j in range(i + 1, len(frame_boxes)):\n",
        "            if iou(frame_boxes[i], frame_boxes[j]) > 0:\n",
        "                # If overlapping, apply a shift\n",
        "                frame_boxes[j] = shift_bbox(frame_boxes[j])\n",
        "        adjusted_boxes.append(scale_bbox(frame_boxes[i]))\n",
        "    return adjusted_boxes\n",
        "\n",
        "# Mutate by scaling and shifting bounding boxes to avoid overlaps\n",
        "def mutate(individual, mutation_rate=0.05, max_shift=3, max_bbox_shift=3):\n",
        "    mutated = deepcopy(individual)\n",
        "    frames = {}\n",
        "\n",
        "    for track in mutated:\n",
        "        for obj in track:\n",
        "            frame = obj['Frame']\n",
        "            if frame not in frames:\n",
        "                frames[frame] = []\n",
        "            frames[frame].append(obj['Bounding Box'])\n",
        "\n",
        "    # Adjust BBoxes in each frame to avoid overlap\n",
        "    for frame, boxes in frames.items():\n",
        "        frames[frame] = adjust_frame_bboxes(boxes)\n",
        "\n",
        "    # Apply adjusted BBoxes back to tracks\n",
        "    for track in mutated:\n",
        "        for obj in track:\n",
        "            frame = obj['Frame']\n",
        "            obj['Bounding Box'] = frames[frame].pop(0)\n",
        "\n",
        "    return mutated\n",
        "\n",
        "# Simulated Annealing with track shifting and enhanced mutation\n",
        "def simulated_annealing(database, initial_temperature=1000, cooling_rate=0.995, mutation_rate=0.1, max_generations=100):\n",
        "    # Initialize population with a single solution\n",
        "    current_solution = deepcopy(database)\n",
        "\n",
        "    # Calculate area and velocity for the initial solution\n",
        "    current_solution = [calculate_velocity(track) for track in current_solution]\n",
        "    current_fitness = fitness(current_solution)\n",
        "    temperature = initial_temperature\n",
        "\n",
        "    for generation in range(max_generations):\n",
        "        # Mutate the current solution\n",
        "        new_solution = mutate(current_solution, mutation_rate)\n",
        "\n",
        "        # Calculate area and velocity for the new solution\n",
        "        new_solution = [calculate_velocity(track) for track in new_solution]\n",
        "        new_fitness = fitness(new_solution)\n",
        "\n",
        "        # Calculate the acceptance probability\n",
        "        if new_fitness > current_fitness:\n",
        "            current_solution = new_solution\n",
        "            current_fitness = new_fitness\n",
        "        else:\n",
        "            acceptance_probability = math.exp((new_fitness - current_fitness) / temperature)\n",
        "            if random.random() < acceptance_probability:\n",
        "                current_solution = new_solution\n",
        "                current_fitness = new_fitness\n",
        "\n",
        "        # Cool down the temperature\n",
        "        temperature *= cooling_rate\n",
        "        if generation % 50 == 0 or generation == max_generations - 1:\n",
        "            print(f\"Generation {generation}, Fitness: {current_fitness:.4f}, Temperature: {temperature:.2f}\")\n",
        "\n",
        "    return current_solution\n",
        "\n",
        "# Main script to load the CSV and run simulated annealing\n",
        "if __name__ == \"__main__\":\n",
        "    csv_file = '/content/object_tracks.csv'  # Replace with the path to your CSV file\n",
        "    tracking_database = load_tracking_database(csv_file)\n",
        "\n",
        "    # Group the tracks by Track ID\n",
        "    grouped_tracks = group_tracks_by_id(tracking_database)\n",
        "\n",
        "    # Run simulated annealing on the grouped tracks\n",
        "    best_tracks = simulated_annealing(grouped_tracks)\n",
        "\n",
        "    # Flatten the list of tracks for output\n",
        "    optimized_data = []\n",
        "    for track in best_tracks:\n",
        "        optimized_data.extend(track)\n",
        "\n",
        "    # Convert back to DataFrame for saving or further processing\n",
        "    optimized_df = pd.DataFrame(optimized_data)\n",
        "\n",
        "    # Save optimized data with area and velocity included\n",
        "    optimized_df.to_csv('/content/optimized_person_tracks1.csv', index=False)\n",
        "\n",
        "    print(\"Optimized Person Tracking Data Saved to 'optimized_person_tracks.csv'\")\n",
        "    print(optimized_df.head())\n"
      ],
      "metadata": {
        "id": "4LzDGq-WOsVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b-77JXp7Q2oz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}